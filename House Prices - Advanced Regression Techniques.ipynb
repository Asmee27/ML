{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "u_mjk7fAYz2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "K2nDuORZYvjM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "y = np.log1p(train[\"SalePrice\"])   # log(1 + SalePrice)\n",
        "train.drop(\"SalePrice\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Sph4oat2Y9V3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([train, test], axis=0)\n"
      ],
      "metadata": {
        "id": "1271qdK4Y_SU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age of house\n",
        "all_data[\"HouseAge\"] = all_data[\"YrSold\"] - all_data[\"YearBuilt\"]\n",
        "all_data[\"RemodAge\"] = all_data[\"YrSold\"] - all_data[\"YearRemodAdd\"]\n",
        "\n",
        "# Save test_ids before dropping 'Id' from all_data\n",
        "# Use original 'test' DataFrame for robust extraction of test_ids\n",
        "test_ids = test[\"Id\"]\n",
        "\n",
        "# Drop Id from all_data\n",
        "# Use errors='ignore' for robustness against multiple cell executions\n",
        "all_data.drop(\"Id\", axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# Split all_data back into train and test after feature engineering\n",
        "# Use len(y) to get the original length of the training data reliably\n",
        "original_train_len = len(y)\n",
        "train = all_data.iloc[:original_train_len, :] # Redefine train with engineered features\n",
        "test = all_data.iloc[original_train_len:, :] # Redefine test with engineered features"
      ],
      "metadata": {
        "id": "PTc6B0D0ZBSJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_features = train.select_dtypes(include=[\"object\"]).columns"
      ],
      "metadata": {
        "id": "bzb3VLGKZDPg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\",\n",
        "     __import__(\"sklearn\").preprocessing.OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_features),\n",
        "    (\"cat\", cat_pipeline, cat_features)\n",
        "])\n"
      ],
      "metadata": {
        "id": "UZr_j1quZFIj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "cFI5vJqIZG_C"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"model\", model)\n",
        "])\n"
      ],
      "metadata": {
        "id": "HurAjm8JZJS8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rmse = np.sqrt(-cross_val_score(\n",
        "    pipeline, train, y,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=kf\n",
        "))\n",
        "\n",
        "print(\"CV RMSE:\", rmse.mean())\n"
      ],
      "metadata": {
        "id": "X7r9P17RZLX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(train, y)\n",
        "\n",
        "preds = pipeline.predict(test)\n",
        "preds = np.expm1(preds)   # reverse log\n"
      ],
      "metadata": {
        "id": "__Gc139rZNQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'preds' not in locals() and 'preds' not in globals():\n",
        "    raise NameError(\"The 'preds' variable is not defined. Please ensure the model fitting and prediction cell (__Gc139rZNQS) has been executed before running this cell.\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"Id\": test_ids,\n",
        "    \"SalePrice\": preds\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "XdPejm0nZPMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50af9ee0"
      },
      "source": [
        "# Task\n",
        "Generate advanced features (e.g., polynomial, interaction terms, or domain-specific aggregations) from the existing features in the `all_data` DataFrame. Ensure to update the `num_features` and `cat_features` lists to reflect these new features, and then re-evaluate the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2139683"
      },
      "source": [
        "## Generate Advanced Features\n",
        "\n",
        "### Subtask:\n",
        "Create more complex features by combining existing ones, such as total living area, total bathrooms, and interaction terms. After generating these new features, ensure to update the `num_features` and `cat_features` lists to reflect them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7284eeed"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create new advanced features like `TotalSF`, `TotalBath`, `OverallQual_GrLivArea`, and `HasPool` by combining existing columns in the `all_data` DataFrame. After creating these new features, I will re-split the `all_data` DataFrame into `train` and `test` based on `original_train_len` to ensure both DataFrames include the newly engineered features. Finally, I will update the `num_features` and `cat_features` lists based on the data types in the re-split `train` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bbf4215"
      },
      "source": [
        "all_data[\"TotalSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n",
        "all_data[\"TotalBath\"] = (\n",
        "    all_data[\"FullBath\"]\n",
        "    + 0.5 * all_data[\"HalfBath\"]\n",
        "    + all_data[\"BsmtFullBath\"]\n",
        "    + 0.5 * all_data[\"BsmtHalfBath\"]\n",
        ")\n",
        "all_data[\"OverallQual_GrLivArea\"] = all_data[\"OverallQual\"] * all_data[\"GrLivArea\"]\n",
        "all_data[\"HasPool\"] = (all_data[\"PoolArea\"] > 0).astype(int)\n",
        "\n",
        "# Re-split all_data back into train and test after feature engineering\n",
        "train = all_data.iloc[:original_train_len, :]\n",
        "test = all_data.iloc[original_train_len:, :]\n",
        "\n",
        "# Update feature lists\n",
        "num_features = train.select_dtypes(include=np.number).columns\n",
        "cat_features = train.select_dtypes(include=object).columns\n",
        "\n",
        "print(\"Advanced features created and train/test DataFrames updated.\")\n",
        "print(f\"New num_features count: {len(num_features)}\")\n",
        "print(f\"New cat_features count: {len(cat_features)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aa29267"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `num_features` and `cat_features` lists now updated to include the newly engineered features, I need to redefine the `preprocessor` to incorporate these changes. This ensures that the data transformation steps (imputation, scaling, one-hot encoding) are applied correctly to all relevant features, including the new ones, before the model is trained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24be46e0"
      },
      "source": [
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\",\n",
        "     __import__(\"sklearn\").preprocessing.OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_features),\n",
        "    (\"cat\", cat_pipeline, cat_features)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f33c861"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `preprocessor` updated to handle the new features, the model pipeline now needs to be re-assembled to incorporate this updated `preprocessor`. This ensures that the entire data processing and modeling workflow uses the most current feature set before model evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17c2e7d9"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"model\", model)\n",
        "])\n",
        "\n",
        "print(\"Model pipeline re-assembled with updated preprocessor.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2db04b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the pipeline has been re-assembled with the updated preprocessor and new features, I need to re-evaluate the model's performance using cross-validation. This will allow us to assess if the advanced features have improved the model's predictive power.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "524c21e8"
      },
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rmse = np.sqrt(-cross_val_score(\n",
        "    pipeline, train, y,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=kf\n",
        "))\n",
        "\n",
        "print(\"CV RMSE (after advanced features):\", rmse.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9652d710"
      },
      "source": [
        "## Implement Hyperparameter Tuning\n",
        "\n",
        "### Subtask:\n",
        "Utilize techniques like GridSearchCV or RandomizedSearchCV from scikit-learn to systematically search for the optimal hyperparameters for the `GradientBoostingRegressor`. This involves defining a grid of parameter values and using cross-validation to find the combination that yields the best performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaf943cd"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement hyperparameter tuning, I will import `GridSearchCV`, define a parameter grid for the `GradientBoostingRegressor`, instantiate `GridSearchCV` with the defined pipeline and parameters, fit it to the training data, and then print the best parameters and corresponding RMSE score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2406465c"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 300, 500],\n",
        "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'model__max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    n_jobs=-1, # Use all available cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Starting GridSearchCV...\")\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(train, y)\n",
        "\n",
        "print(\"GridSearchCV completed.\")\n",
        "# Print the best parameters\n",
        "print(\"Best hyperparameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best cross-validation score (RMSE)\n",
        "best_rmse = np.sqrt(-grid_search.best_score_)\n",
        "print(\"Best CV RMSE:\", best_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d87c461"
      },
      "source": [
        "## Re-evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Re-run the cross-validation step to measure the improved RMSE using the optimal hyperparameters found during GridSearchCV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9edebff7"
      },
      "source": [
        "**Reasoning**:\n",
        "After running `GridSearchCV` to find the optimal hyperparameters, I will extract the best estimator from the `grid_search` object. This best estimator is a pipeline that includes the preprocessor and the `GradientBoostingRegressor` with the tuned parameters. I will then use this `best_estimator_` to perform a 5-fold cross-validation on the training data (`train` and `y`) to assess its performance with the optimized settings. Finally, I will calculate and print the mean of the Root Mean Squared Error (RMSE) obtained from this cross-validation, allowing for a direct comparison with previous evaluations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c116d3b8"
      },
      "source": [
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rmse_tuned = np.sqrt(-cross_val_score(\n",
        "    best_pipeline, train, y,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=kf\n",
        "))\n",
        "\n",
        "print(\"CV RMSE (after hyperparameter tuning):\", rmse_tuned.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8d5afa"
      },
      "source": [
        "## Generate New Submission File\n",
        "\n",
        "### Subtask:\n",
        "Train the final model with the optimal hyperparameters on the entire training dataset (with new features) and generate a new prediction file for submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f686514"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the optimal hyperparameters have been found and the model performance re-evaluated, I will train the `best_pipeline` on the entire training data, generate predictions on the test set, reverse the log transformation, and create the submission file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49f741e2"
      },
      "source": [
        "best_pipeline.fit(train, y)\n",
        "\n",
        "preds_tuned = best_pipeline.predict(test)\n",
        "preds_tuned = np.expm1(preds_tuned)   # reverse log\n",
        "\n",
        "submission_tuned = pd.DataFrame({\n",
        "    \"Id\": test_ids,\n",
        "    \"SalePrice\": preds_tuned\n",
        "})\n",
        "\n",
        "submission_tuned.to_csv(\"submission_tuned.csv\", index=False)\n",
        "\n",
        "print(\"New submission file 'submission_tuned.csv' created.\")\n",
        "submission_tuned.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb461a7"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the improvements made to the model, including the impact of new features and hyperparameter tuning on the model's performance and ranking potential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3b96ff"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Advanced Feature Engineering:** Four new features (`TotalSF`, `TotalBath`, `OverallQual_GrLivArea`, and `HasPool`) were created. This updated the feature set to 42 numerical and 43 categorical features.\n",
        "*   **Performance Post-Feature Engineering:** After incorporating advanced features, the model achieved an average cross-validation RMSE of approximately 0.1265.\n",
        "*   **Hyperparameter Tuning Impact:** Hyperparameter tuning using `GridSearchCV` on the `GradientBoostingRegressor` identified optimal settings, further reducing the average cross-validation RMSE to approximately 0.1254. This represents an improvement of about 0.0011 over the model with only advanced features.\n",
        "*   **Final Model & Submission:** A final model, incorporating both advanced features and optimal hyperparameters, was trained on the entire dataset, and a submission file named 'submission\\_tuned.csv' was successfully generated.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The combination of targeted feature engineering and systematic hyperparameter tuning significantly enhanced the model's predictive performance, as evidenced by the consistent reduction in RMSE.\n",
        "*   Future efforts could explore additional complex feature interactions, alternative regression models, or ensemble techniques to potentially achieve even greater accuracy.\n"
      ]
    }
  ]
}